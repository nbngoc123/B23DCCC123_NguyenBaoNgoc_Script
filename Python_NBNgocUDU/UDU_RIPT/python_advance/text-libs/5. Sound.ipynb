{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkzUUgmvA33i"
   },
   "source": [
    "# Speech Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOk_w504A6DB"
   },
   "source": [
    "## Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q31-BAGCBtMx",
    "outputId": "ccd17cee-e9d0-4bd4-c62c-465a90d2bd7b"
   },
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "skaSS0baA5Zj",
    "outputId": "7e2f2175-f1f4-414f-823c-6b7092c4cbcb"
   },
   "outputs": [],
   "source": [
    "# 1. read .wav \n",
    "import librosa\n",
    "\n",
    "filename = librosa.example('nutcracker')\n",
    "# Load the audio as a waveform `y`\n",
    "# Store the sampling rate as `sr`\n",
    "audio, sr = librosa.load(filename)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "time = np.linspace(0, audio.shape[0] / sr, num=audio.shape[0]) \n",
    "plt.plot(time, audio, color=\"blue\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (quantized)\")\n",
    "plt.title(\"Wav file visualization\")\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "Wr5J-l0dBYEW",
    "outputId": "ae15153d-085d-485d-f35b-a87d301cdd8c"
   },
   "outputs": [],
   "source": [
    "# 2. mel spectrogram\n",
    "import librosa\n",
    "filename = librosa.example('nutcracker')\n",
    "# Load the audio as a waveform `y`\n",
    "# Store the sampling rate as `sr`\n",
    "audio, sr = librosa.load(filename)\n",
    "S = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                         y_axis='mel', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuRDKKCsBi1C"
   },
   "source": [
    "## Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fx0AdVAfB4tC",
    "outputId": "9d58cd90-c91f-4537-a1d6-0aee1ec15a18"
   },
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "4r8tI9yABlNG",
    "outputId": "f295d71a-d7db-4cba-cbf7-4b9a0fb20f07"
   },
   "outputs": [],
   "source": [
    "# 3. read wav \n",
    "from scipy.io import wavfile\n",
    "\n",
    "wav_filepath = 'data.wav'\n",
    "sampling_rate, data = wavfile.read(wav_filepath) \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "time = np.linspace(0, data.shape[0] / sampling_rate, num=data.shape[0]) \n",
    "\n",
    "plt.plot(time, data, color=\"green\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (quantized)\")\n",
    "plt.title(\"Wav file visualization\")\n",
    "plt.axhline(y=0, color='orange', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7SeBzJZFqNX"
   },
   "source": [
    "## SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25wU7ipLFsA4",
    "outputId": "67edff3e-dc8f-4b8e-8a37-68c6b5970adc"
   },
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JchGertqFtkF",
    "outputId": "5dd08764-e68b-4356-f244-ea06b8d818e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'speech_recognition.AudioData'>\n",
      "Audio transcription:  hiện ra tận phía xa\n"
     ]
    }
   ],
   "source": [
    "# automatic speech recognition\n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "audio_filename = 'data.wav'\n",
    "\n",
    "\n",
    "my_audio = sr.AudioFile(audio_filename)\n",
    "with my_audio as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "print(type(audio))\n",
    "your_speech = r.recognize_google(audio, language=\"vi-VN\")\n",
    "print(\"Audio transcription: \", your_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HdXdDYhPYo7"
   },
   "source": [
    "## gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lNAjaINPpQA",
    "outputId": "65064792-08cc-4757-f189-9dad05db4a68"
   },
   "outputs": [],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "OrS4fKWLPZXR",
    "outputId": "e329b1ee-2558-4522-db2b-0cfa724706e5"
   },
   "outputs": [],
   "source": [
    "# 4. text to speech (colab)\n",
    "from gtts import gTTS\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "lang='vi'\n",
    "output_filename = 'record.mp3'\n",
    "content = \"xin chào mọi người\"\n",
    "output = gTTS(content, lang=lang, slow=False) # text to speech\n",
    "\n",
    "output.save(output_filename) # save google audio to a file\n",
    "data, sr = librosa.load(output_filename) # load google audio using librosa library\n",
    "\n",
    "import IPython\n",
    "IPython.display.display(IPython.display.Audio(np.transpose(data), rate=sr)) # display audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP libraries.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
